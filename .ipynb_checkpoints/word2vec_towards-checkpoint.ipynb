{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_token = [[line.split()[i] for i in range(len(line.split()))] for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is', 'a', 'king'],\n",
       " ['she', 'is', 'a', 'queen'],\n",
       " ['he', 'is', 'a', 'man'],\n",
       " ['she', 'is', 'a', 'woman'],\n",
       " ['warsaw', 'is', 'poland', 'capital'],\n",
       " ['berlin', 'is', 'germany', 'capital'],\n",
       " ['paris', 'is', 'france', 'capital']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(corpus_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = dictionary.token2id\n",
    "idx2word = {value: key for key, value in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(dictionary)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make pairs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [1, 0],\n",
       "       [2, 1],\n",
       "       [2, 0],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_pairs(window_size, corpus_token, word2idx):\n",
    "    idx_pairs = []\n",
    "\n",
    "    #for each sentence\n",
    "    for line in corpus_token:\n",
    "        idxs = [word2idx[indx] for indx in line]\n",
    "\n",
    "        #for each word, as center\n",
    "        for center_word_pos in range(len(idxs)):\n",
    "            #check window size:\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "\n",
    "                if context_word_pos < 0 or context_word_pos >= len(idxs) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                #append (center_wobrd_idx, context_word_idx)\n",
    "                idx_pairs.append((idxs[center_word_pos], idxs[context_word_pos]))\n",
    "    return np.array(idx_pairs)   \n",
    "    \n",
    "idx_pairs = make_pairs(2, corpus_token, word2idx)\n",
    "idx_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.x = pairs[:, 0]\n",
    "        self.y = pairs[:, 1]\n",
    "        self.size = len(pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(idx_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make real Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(idx_pairs)\n",
    "train_loader = DataLoader(train_data, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1]), tensor([2]))"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, v_size, dim_embed = 5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(v_size, dim_embed)\n",
    "        self.fc1 = nn.Linear(dim_embed, v_size)\n",
    "        \n",
    "    #x is a word\n",
    "    def forward(self, x):\n",
    "        embed = self.embed(x)\n",
    "        out = self.fc1(embed)\n",
    "        out = F.log_softmax(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def get_embed(self, idx):\n",
    "        return self.embed.weight.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embed): Embedding(15, 5)\n",
       "  (fc1): Linear(in_features=5, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net(v_size = vocabulary_size)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data, target in train_loader:\n",
    "    break\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camilo/.local/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8366, -2.9667, -3.4827, -3.0368, -3.2549, -2.4193, -2.6510, -2.6643,\n",
       "         -2.7437, -2.3619, -2.2570, -2.5692, -3.3585, -2.9064, -2.1706]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 5])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.embed.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camilo/.local/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 2.82853593315397\n",
      "Epoch: 100  Loss: 2.3131393619946072\n",
      "Epoch: 200  Loss: 2.1598001599311827\n",
      "Epoch: 300  Loss: 2.0655481048992703\n",
      "Epoch: 400  Loss: 1.9917369825499398\n",
      "Epoch: 500  Loss: 1.9294971261705671\n",
      "Epoch: 600  Loss: 1.8757747207369124\n",
      "Epoch: 700  Loss: 1.8294639331953866\n",
      "Epoch: 800  Loss: 1.7899069036756243\n",
      "Epoch: 900  Loss: 1.7563524637903487\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "embed_size = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "net = Net(v_size = vocabulary_size, dim_embed=embed_size)\n",
    "net.cuda()\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.001)\n",
    "\n",
    "\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    loss_val = 0\n",
    "    for context, target in train_loader:\n",
    "        context = context.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        y_pred = net(context)\n",
    "        loss = criterion(y_pred, target)\n",
    "        loss_val += loss.item()\n",
    "        \n",
    "        #parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    loss_val/= train_data.size\n",
    "    losses.append(loss_val)\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        \n",
    "    \n",
    "        print(f'Epoch: {epoch}  Loss: {loss_val}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7693,  1.5804,  1.1324,  0.0974, -0.6036], device='cuda:0')"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_embed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7693,  1.5804,  1.1324,  0.0974, -0.6036],\n",
       "        [-0.5919, -1.7285, -0.6342, -0.9270,  0.3097],\n",
       "        [ 1.4267,  1.2348, -0.3616, -1.7160, -0.5362],\n",
       "        [-0.6727, -1.0930, -0.0364, -1.3229, -0.2836],\n",
       "        [ 0.1836,  0.3944, -0.5896,  0.1432,  1.7754],\n",
       "        [-0.1902, -0.4395, -1.0029, -0.1735,  0.8215],\n",
       "        [-1.2302,  0.5870,  0.0616, -0.9032,  0.7505],\n",
       "        [-0.5739, -1.2524, -0.0195, -1.3695,  0.5792],\n",
       "        [ 0.9533, -2.0017,  1.4673,  0.0081,  0.5492],\n",
       "        [-1.0563, -0.8481,  0.0905, -0.3114, -2.1589],\n",
       "        [-0.0128,  0.2989, -0.0523,  0.2534, -0.1739],\n",
       "        [ 0.6727, -1.5024, -0.8055,  0.6373, -0.1328],\n",
       "        [ 0.5118, -0.6419, -0.9790,  0.4873,  0.4657],\n",
       "        [ 0.0172, -0.1245, -0.5040, -0.4147, -0.7439],\n",
       "        [ 1.5319, -0.6502,  0.4326,  1.2661,  0.5440]], device='cuda:0')"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.embed.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = net.embed.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save weights and retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camilo/.local/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 2.971930813789368\n",
      "Epoch: 100  Loss: 2.147501311983381\n",
      "Epoch: 200  Loss: 1.927611494064331\n",
      "Epoch: 300  Loss: 1.8337325726236615\n",
      "Epoch: 400  Loss: 1.779344265801566\n",
      "Epoch: 500  Loss: 1.7423444373267039\n",
      "Epoch: 600  Loss: 1.7147527166775294\n",
      "Epoch: 700  Loss: 1.692982372215816\n",
      "Epoch: 800  Loss: 1.6751613514763968\n",
      "Epoch: 900  Loss: 1.6601911698068892\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "embed_size = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "net = Net(v_size = vocabulary_size, dim_embed=embed_size)\n",
    "net.embed.weight.data = weights\n",
    "net.cuda()\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.001)\n",
    "\n",
    "\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    loss_val = 0\n",
    "    for context, target in train_loader:\n",
    "        context = context.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        y_pred = net(context)\n",
    "        loss = criterion(y_pred, target)\n",
    "        loss_val += loss.item()\n",
    "        \n",
    "        #parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_val/= train_data.size\n",
    "        \n",
    "    losses.append(loss_val)\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        \n",
    "    \n",
    "        print(f'Epoch: {epoch}  Loss: {loss_val}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dictionary of word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2vec = {key: net.get_embed(key).cpu().numpy() for key in idx2word.keys()}\n",
    "word2vec = {key: idx2vec[value] for key, value in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'he', 'is']"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word2vec)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(word, k = 5, idx2vec = idx2vec):\n",
    "    vector_word = word2vec[word]\n",
    "    \n",
    "    vectors = [value for key, value in word2vec.items()]\n",
    "    target_word = np.tile(vector_word, (len(vectors), 1))\n",
    "    dist = np.linalg.norm(vectors - target_word, axis = 1 )\n",
    "    \n",
    "    #k_closers\n",
    "    k_indexes = np.argsort(dist)[1:k]\n",
    "    k_words = [idx2word[index] for index in k_indexes]\n",
    "    \n",
    "    print(k_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['woman', 'king', 'she', 'man']\n"
     ]
    }
   ],
   "source": [
    "find_closest('he', k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
